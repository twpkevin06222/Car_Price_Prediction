{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c93d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import utils\n",
    "from scipy.stats import skew\n",
    "import math\n",
    "import re\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84095f7f",
   "metadata": {},
   "source": [
    "This notebook attempts to provide an overview about insights on data understanding of the provided dataset.\n",
    "This includes checking anomalies of the dataset, spotting errors and potential concerns of the data.\n",
    "* Objective: Predict vehicle sale prices (Sold_Amount)\n",
    "* Constraints (not applicable): AvgWholesale, AvgRetail, GoodWholesale, GoodRetail, TradeMin, TradeMax, PrivateMax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89e8bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a57cf",
   "metadata": {},
   "source": [
    "# Content:\n",
    "1. Data Entry\n",
    "2. Data preprocessing\n",
    "    - 2.1 Evaluating Target Variables\n",
    "    - 2.2 Removing Field of Constraints\n",
    "    - 2.3 Handling NULL Values\n",
    "    - 2.4 Handling Potential Similar Columns\n",
    "    - 2.5 Removing Special Characters\n",
    "    - 2.6 Correlation Analysis\n",
    "    - 2.7 Imputation\n",
    "        - 2.7.1 Binning Data\n",
    "        - 2.7.2 Numerical Data\n",
    "        - 2.7.3 Categorical Data\n",
    "    - 2.8 Feature Selection\n",
    "3. Finishing Touch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a9396d",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658a85b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aed9ae",
   "metadata": {},
   "source": [
    "# 1. Data Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "137c9375",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DatiumSample.rpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-557510c16e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DatiumSample.rpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/envs/tfenv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;31m# default to avoid a ValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tfenv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tfenv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tfenv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tfenv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/tfenv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DatiumSample.rpt'"
     ]
    }
   ],
   "source": [
    "# load the data set\n",
    "ds = pd.read_table('/home/kevinteng/Desktop/DatiumSample.rpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e6ad4",
   "metadata": {},
   "source": [
    "Check the number of rows and columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e74fa41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows, cols = ds.shape\n",
    "print(\"There are {} rows.\".format(rows))\n",
    "print(\"There are {} columns.\".format(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data type\n",
    "ds.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f937d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7f93f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Data Preprocessing <a class=\"anchor\" id=\"dataPre\"></a>\n",
    "This section attempts to improve the quality of the data via finding the caveats of the data set,\n",
    "manipulating and dropping data before the input pipeline of the model. This section also deals with feature selections where we attempt to purge features that are least relevent to the traget variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ef420",
   "metadata": {},
   "source": [
    "## 2.1 Evaluating Target Variables\n",
    "Since the objective is to predict the vehicle sold price, which is \"Sold_Amount\".\n",
    "It is crucial to check if \"Sold_Amount\" is valid for all records, else there would be no target for predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Sold_Amount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d3a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records with NULL price: \", ds['Sold_Amount'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409234a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the row indices where pice is NULL\n",
    "price_isna_idx = np.where(ds['Sold_Amount'].isna()==True)[0]\n",
    "# remove the rows where target price is not available \n",
    "ds.drop(price_isna_idx, axis=0, inplace=True)\n",
    "print(\"Number of records with NULL price: \", ds['Sold_Amount'].isna().sum())\n",
    "print(\"-\"*50)\n",
    "rows, cols = ds.shape\n",
    "print(\"There are {} rows.\".format(rows))\n",
    "print(\"There are {} columns.\".format(cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507194db",
   "metadata": {},
   "source": [
    "Before clipping the values of the target variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "price_dist = sns.displot(ds['Sold_Amount'], kde=True)\n",
    "price_dist.fig.set_size_inches(8,5)\n",
    "print(\"Skewness: \", ds['Sold_Amount'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b9d7a",
   "metadata": {},
   "source": [
    "Looking at the distribution of car price, the distribution is right-skewed with some outliers on car price >50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f9ab0e",
   "metadata": {},
   "source": [
    "Remove outliers by clipping the price value up to 99th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[ds.Sold_Amount < ds.Sold_Amount.quantile(0.99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ea35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['Sold_Amount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c3053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "price_dist = sns.displot(ds['Sold_Amount'], kde=True)\n",
    "price_dist.fig.set_size_inches(8,5)\n",
    "print(\"Skewness: \", ds['Sold_Amount'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf7155e",
   "metadata": {},
   "source": [
    "The distribution looks much better. Alternatively, lets see how log transformation will do to the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687420c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using log1p transformation with apply log(1+x) to reduce error for min. value of 0 \n",
    "y = ds['Sold_Amount'].copy()\n",
    "log_y = np.log1p(y)\n",
    "price_dist = sns.displot(log_y, kde=True)\n",
    "price_dist.fig.set_size_inches(8,5)\n",
    "print(\"Skewness: \", log_y.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c38a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fc556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicate rows\n",
    "print(\"Number of duplicate rows: \",ds.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7062b3d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53812c19",
   "metadata": {},
   "source": [
    "## 2.2 Removing Field of Constraints\n",
    "Drop inapplicable field: 'AvgWholesale', 'AvgRetail', 'GoodWholesale', 'GoodRetail', 'TradeMin', 'TradeMax', 'PrivateMax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad2650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# field to be dropped\n",
    "constraints = ['AvgWholesale', 'AvgRetail', 'GoodWholesale', \n",
    "               'GoodRetail', 'TradeMin', 'TradeMax', 'PrivateMax'] \n",
    "ds.drop(columns = constraints, inplace = True)\n",
    "# sanity check for the dimensionality of the data set\n",
    "print(\"There are {} columns.\".format(ds.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5223b74c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0720a3d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.3 Handling NULL Values\n",
    "In this part, we deal with fields having NULL values exceeding a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03709afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part, the number of missing values for each fields is validated and ranked.\n",
    "null_count_arr = ds.isna().sum()\n",
    "print(null_count_arr.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd009509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of missing values by top 80 features \n",
    "null_ratio = (null_count_arr.sort_values(ascending=False)[:80]/rows)*100\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.xticks(rotation = '90')\n",
    "sns.barplot(x = null_ratio.index, y = null_ratio)\n",
    "plt.xlabel('Features', fontsize = 15)\n",
    "plt.ylabel('Missing Values (%)', fontsize = 5)\n",
    "plt.title('Percentage of Missing Values by Features', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3891b",
   "metadata": {},
   "source": [
    "By observing the above information, there are quite a number of fields that contain \n",
    "NULL values equivalent to the number of records. These fields will not provide \n",
    "information to the predictive model. Therefore, we specify a certain threshold where\n",
    "the NULL values exceed a certain percentage of the total records, the field is concurrently dropped. In this case, we defined the threshold percentage as 80%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5096d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of NULL values that exceed the total number of records  \n",
    "pct = 0.8\n",
    "# get the index where NULL value is larger than certain (pct) of the records\n",
    "null_thr_idx = np.where(null_count_arr > int(rows*pct))[0]\n",
    "print(\"Number of columns to be dropped: \", null_thr_idx.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57909b",
   "metadata": {},
   "source": [
    "Dropping the fields of NULL value exceeding 80% of the total records.\n",
    "Drop 'Description' field because it gives a little insight and it is not helpful for predicting the target price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d46a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which particular field will need to be dropped\n",
    "# by mapping the indices to the respective field \n",
    "ds.drop(columns = np.take(ds.columns, null_thr_idx).tolist(), inplace = True)\n",
    "# ds['is_sport'] = ds['Description'].apply(lambda x: 1 if 'sport' in x.lower() else 0)\n",
    "ds.drop(columns = \"Description\", inplace = True)\n",
    "print(\"There are {} columns.\".format(ds.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ff65d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ab57a",
   "metadata": {},
   "source": [
    "## 2.4 Handling Potential Similar Columns\n",
    "Remove certain fields that are short form representations of a given fields these field contains the keyword \"Code\" inside:\n",
    "- Example:  \n",
    "\"MakeCode\" field is the abbreviation of \"MakeCode\"  \n",
    "\"FamilyCode\" field is the abbreviation of \"Model\"  \n",
    "\"DriveCode\" field is the abbreviation of \"DriveDescription\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search fields that contains wildcard \"Code\"\n",
    "match_id_list = utils.string_search(ds.columns.to_list(), \"*Code\")\n",
    "print([ds.columns.to_list()[i] for i in match_id_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632c1a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# \"ModelCode\" is omitted because the column is unique\n",
    "ds.drop(columns = ['MakeCode', 'FamilyCode', 'DriveCode'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4dd2e8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf07b4f",
   "metadata": {},
   "source": [
    "## 2.5 Removing Special Characters\n",
    "After dropping all irrelevant fields, we need to run a check for potential special characters inside records. Intuitively at first glance, the field for 'EngineNum' contains special characters, such as #, -, ?. Special characters such as \"., /, :, -, <, >, (, )\" should be omiited.\n",
    "- \".\" is used in decimals\n",
    "- \"/\" is used in date or special representations\n",
    "- \":\" is used in timestamp\n",
    "- \"<\" is symbol as less than \n",
    "- \">\" is symbol as larger than \n",
    "- \"()\" brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26226d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds['EngineNum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81392e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do! better way of removing special character \n",
    "special_char = \"[^A-Za-z0-9:/.<()>\\s]\"\n",
    "# let's check which field have special characters and how many rows of it\n",
    "special_char_field = utils.special_char_field_checker(ds, special_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c6044",
   "metadata": {},
   "source": [
    "Proceed to remove the special character for each fields and do a sanity check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special character for the appointed column \n",
    "for f in special_char_field:\n",
    "    # for some reason, .replace() works on data type 'object' but not 'string'\n",
    "    ds[f].replace(special_char, '', regex=True, inplace=True) \n",
    "# sanity check    \n",
    "special_char_field = utils.special_char_field_checker(ds, special_char)\n",
    "# no output means that special character has been remove!\n",
    "# now we validate the records after removing special characters\n",
    "print(ds['EngineNum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f483356",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3d586",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "## 2.6 Correlation Analysis\n",
    "This section will attempt to visualise the categorical data with correlation matrix on heatmap\n",
    "and trim the highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cef7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Pearson method for correlation\n",
    "cor_matrix = ds.corr('pearson').abs()\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(cor_matrix, cmap = 'viridis')\n",
    "plt.colorbar()\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1d6af",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check multicollinearity among the independent variable by plotting the heatmap of correlation matrix of the numerical data.\n",
    "From the distribution of the heatmap, we can observe that there are some variables that has high correlation dependencies.\n",
    "We try to trim high correlation dependencies given a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2e2f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# since correlation matrices are symmetrical, we take the upper triangle part to create masking\n",
    "upper_tri_mask = np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool)\n",
    "upper_tri = cor_matrix.where(upper_tri_mask)\n",
    "# retrieve the column that has correlation greater than a given threshold\n",
    "high_cor_col = [col for col in upper_tri.columns if any(upper_tri[col]>0.8)]\n",
    "print('Fields with high correlation:')\n",
    "print(high_cor_col)\n",
    "print()\n",
    "print(\"The number of column to be dropped: \", len(high_cor_col))\n",
    "# drop columns with high correlation!\n",
    "ds.drop(columns = high_cor_col, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177d59c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot post processed correlation matrix\n",
    "post_cor_matrix = ds.corr('pearson').abs()\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(post_cor_matrix, cmap = 'viridis')\n",
    "plt.colorbar()\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5ae86",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As observed from the post processed correlation matrix, the high correlation hot spots has been mitigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94621d53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the number of categorical fields and numerical fields\n",
    "# field indices where column is categorical\n",
    "categorical_col_idx = np.where(ds.dtypes == object)[0]\n",
    "# field indices where column is numerical\n",
    "numerical_col_idx = np.where(ds.dtypes != object)[0]\n",
    "print(\"Number of categorical fields: \",categorical_col_idx.shape[0])\n",
    "print(\"Number of numerical fields: \",numerical_col_idx.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a53de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba081e",
   "metadata": {},
   "source": [
    "## 2.7 Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff4206",
   "metadata": {},
   "source": [
    "This section attempts to replace missing data with substitution.\n",
    "- Mode for categorical data \n",
    "- Mean for numerical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8adc440",
   "metadata": {},
   "source": [
    "We notice that some data fields are of type 'object' which is a mixed of string and numerical data types.  \n",
    "For ease of manipulation the categorical fields of 'object' type should be converted to 'string'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19adf251",
   "metadata": {},
   "source": [
    "Further cleaning for WarrantyKM, we do not want negative values in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d99467",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['WarrantyKM'] = ds['WarrantyKM'].apply(lambda x: utils.check_warranty_km(x))\n",
    "ds['WarrantyKM'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92cbe2",
   "metadata": {},
   "source": [
    "### 2.7.1 Binning Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_col = ds.dtypes[ds.dtypes != object].index\n",
    "numeric_col_unique = {k: len(ds[k].unique()) for k in numeric_col}\n",
    "print(numeric_col_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa60beea",
   "metadata": {},
   "source": [
    "There are some features with fixed values, hence, we need to convert them to categorical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442cfeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_col = ['YearGroup',\n",
    " 'MonthGroup',\n",
    " 'SequenceNum',\n",
    " 'GearNum',\n",
    " 'DoorNum',\n",
    " 'FuelCapacity',\n",
    " 'SeatCapacity',\n",
    " 'ValvesCylinder',\n",
    " 'TowingNoBrakes', # to  be observed\n",
    " 'WarrantyYears',\n",
    " 'FirstServiceKM',\n",
    " 'RegServiceMonths',\n",
    " 'WarrantyKM']\n",
    "\n",
    "# needed to convert categorical columns from object to strings in order for ease of manipulation\n",
    "categorical_col_idx = np.where(ds.dtypes == object)[0]\n",
    "cols_name = [name for name in ds.columns]\n",
    "categorical_cols = [cols_name[i] for i in categorical_col_idx] + tmp_col\n",
    "ds[categorical_cols] = ds[categorical_cols].astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d725f84d",
   "metadata": {},
   "source": [
    "### 2.7.2 Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050760d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill numerical data of NULL value with mean of the data \n",
    "numeric_cols = [c for c in ds.columns if c not in categorical_cols]\n",
    "# fill NULL value with dependencies on 'Make' column \n",
    "ds[numeric_cols] = ds.groupby(['Make'])[numeric_cols]\\\n",
    "    .transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count_arr03 = ds[[c for c in ds.columns if c not in categorical_cols]].isna().sum()\n",
    "print(null_count_arr03.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93560782",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.fillna(ds.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb991a",
   "metadata": {},
   "source": [
    "### 2.7.3 Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71aaed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count_arr04 = ds.isna().sum()\n",
    "# gather the indices of categorical data records\n",
    "null_thr_idx02 = np.where(null_count_arr04!=0)[0]\n",
    "categorical_field = np.take(ds.columns, null_thr_idx02)\n",
    "print('Categorical fields with NULL values:')\n",
    "print()\n",
    "print(categorical_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the categorical data with mode of each field respectively\n",
    "for n in categorical_cols:\n",
    "    ds[n].fillna(ds[n].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e66288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check if the method works\n",
    "print(np.where(ds.isna().sum()!=0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed21b9",
   "metadata": {},
   "source": [
    "Empty list means that there is no NULL value inside the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753db687",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878da212",
   "metadata": {},
   "source": [
    "## 2.8 Skewness of Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a08aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve numerical data columns\n",
    "numeric_cols = [c for c in ds.columns if c not in categorical_cols]\n",
    "# ranked skewed columns\n",
    "skewed_cols = ds[numeric_cols].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "skewed_df = pd.DataFrame({'Skewness': skewed_cols})\n",
    "skewed_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9359ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose top four right skewed columns for log transformation\n",
    "right_skewed_cols = skewed_cols.index[0:4]\n",
    "for r in right_skewed_cols:\n",
    "    ds[right_skewed_cols] = np.log1p(ds[right_skewed_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14fd362",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.9 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aca5de",
   "metadata": {},
   "source": [
    "Check the number of unique classes for categorical fields. This could be useful for feature selections where we can merge or remove the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_name = [name for name in ds.columns]\n",
    "categorical_cols = [cols_name[i] for i in categorical_col_idx]\n",
    "# dictionary to store number of classes for each categorical fields\n",
    "n_class = {}\n",
    "for c in categorical_cols:\n",
    "    val, count = np.unique(ds[c], return_counts=True)\n",
    "    n_class[c]= len(count)\n",
    "print(n_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18821db5",
   "metadata": {},
   "source": [
    "notice that there is only 1 unique class for 'ImportFlag' hence we dropped it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.drop(columns=['ImportFlag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3933988",
   "metadata": {},
   "source": [
    "From the categorical fields, it is observed that there are some features with more than 100 classes. This will cause problem for feature encoding when we use one hot encoding and get_dummies() from Pandas. By using get_dummies() from Pandas this will lead to a whooping of 22k features! (See below). This will be computational expensive to train. \n",
    "  \n",
    "Strategy:\n",
    "- Use label encoding\n",
    "- Prune features for better representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e184ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categorical_col_idx = np.where(ds.dtypes == 'string')[0]\n",
    "cols_name = [name for name in ds.columns]\n",
    "categorical_cols = [cols_name[i] for i in categorical_col_idx] + tmp_col\n",
    "df = pd.get_dummies(ds, columns = categorical_cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee38bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets pick a few features that exceed unique class of 100 and try to interpret them\n",
    "# retrieve the indices of classes from the dictionary where values exceeded 100 \n",
    "class_100_idx = np.where(np.array(list(n_class.values()))>100)[0]\n",
    "print([list(n_class)[i] for i in class_100_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8412d",
   "metadata": {},
   "source": [
    "Interpretation: (tick sign means the features will be dropped) \n",
    "- [ ] 'Model', 'Series', 'BadgeDescription': are good indications for the car and it is assumed that it will contribute to the car price prediction, so it will not be dropped.\n",
    "- [x] 'VIN': is Vehical Identification Number which is unique for specific automobile. We assume that there are other supporting features for this field and it will be dropped.\n",
    "- [x] 'ModelCode' : We assumed that this will not be a helpful feature and it will be dropped.\n",
    "- [x] 'EngineNum': Noisy data and too hard to be cleaned, it will be dropped \n",
    "- [x] 'FontTyreSize', 'RearTyreSize': Since every car will have a font and rear criterion, we can aggregrate them in a column as 'FontRearTyreSize'. In this case, similar operatin will be done on 'FrontRimDesc' and 'RearRimDesc'.\n",
    "- [ ] 'Colour': Colour of choice is an influencing factor and there are many colour spectrum, therefore this feature will be retained.\n",
    "- [x] 'Sold_Date': Should be dropped, because it will not affect the price. \n",
    "* [x] 'Compliance_Date': To be dropped as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5ed3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if front and rear tyre size are the same, we retain the tyre size spec\n",
    "# else, we replace it with the string concatenation of front tyre size + rear tyre size \n",
    "# create mask \n",
    "front_rear_tyre_size_mask = np.where(np.array(ds['FrontTyreSize'] == ds['RearTyreSize']) == True, \n",
    "                                     1, 0)\n",
    "front_rear_tyre_size = np.where(front_rear_tyre_size_mask == 1,\n",
    "                                ds['FrontTyreSize'], \n",
    "                                ds['FrontTyreSize'] + '+' + ds['RearTyreSize'])\n",
    "# similar to front and rear rim desc \n",
    "# create mask\n",
    "front_rear_rim_desc_mask = np.where(np.array(ds['FrontRimDesc'] == ds['RearRimDesc']) == True, \n",
    "                                    1, 0)\n",
    "front_rear_rim_desc = np.where(front_rear_rim_desc_mask == 1,\n",
    "                               ds['FrontRimDesc'], \n",
    "                               ds['FrontRimDesc'] + '+' + ds['RearRimDesc'])\n",
    "# check the merger unique class\n",
    "val_tyre_size, count_tyre_size = np.unique(front_rear_tyre_size, return_counts = True)\n",
    "val_rim_desc, count_rim_desc = np.unique(front_rear_rim_desc, return_counts = True)\n",
    "print(\"Unique class for FrontRearTyreSize: \",len(count_tyre_size))\n",
    "print(\"Unique class for FrontRearRimDesc: \",len(count_rim_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993157ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets replace new data and drop irrelavant features \n",
    "ds_02 = ds.copy()\n",
    "# replacement of new data \n",
    "ds_02['FrontTyreSize'] = front_rear_tyre_size\n",
    "ds_02['FrontRimDesc'] = front_rear_rim_desc\n",
    "# cast the new features to data type string \n",
    "ds_02[['FrontTyreSize', 'FrontRimDesc']] = ds_02[['FrontTyreSize', \n",
    "                                                 'FrontRimDesc']].astype('string')\n",
    "# rename columns after replacement \n",
    "ds_02.rename(columns = {'FrontTyreSize':'FrontRearTyreSize', \n",
    "                        'FrontRimDesc':'FrontRearRimDesc'},\n",
    "            inplace = True)\n",
    "# drop irrelavant features\n",
    "ds_02.drop(columns = ['VIN', 'ModelCode', 'EngineNum', \n",
    "                      'RearTyreSize', 'RearRimDesc',\n",
    "                     'Sold_Date', 'Compliance_Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d076a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all the data types are valid \n",
    "ds_02.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a037a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for the unique class of the features\n",
    "categorical_col_idx = np.where(ds_02.dtypes == 'string')[0]\n",
    "cols_name = [name for name in ds_02.columns]\n",
    "categorical_cols = [cols_name[i] for i in categorical_col_idx]\n",
    "n_class_02 = {}\n",
    "for c in categorical_cols:\n",
    "    val, count = np.unique(ds_02[c], return_counts=True)\n",
    "    n_class_02[c]= len(count)\n",
    "print('Number of categorical data: ', len(categorical_col_idx))\n",
    "print()\n",
    "print(n_class_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c61a4c",
   "metadata": {},
   "source": [
    "'Colour' feature seems like a very noisy feature, lets see how we can deal with it. \n",
    "We can group the colors in to common colours, such as: {gold, white, silver, black, grey, black, blue, red, yellow, orange, green, purple, brown}, else others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66103211",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_colour, count_colour = np.unique(ds_02['Colour'], return_counts = True)\n",
    "colour_df = pd.DataFrame(sorted(zip(count_colour, val_colour), reverse = True),\n",
    "                         columns = ['Count', 'Colour'])\n",
    "colour_df = colour_df.reindex(columns = ['Colour', 'Count'])\n",
    "print(colour_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a216e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_02['Colour'] = ds_02['Colour'].apply(lambda x: utils.strip_colors(x))\n",
    "ds_02['Colour'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data in .csv format for model building\n",
    "ds_02.to_csv('dummies_ds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d626cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the feature columns after feature encoding for get_dummies\n",
    "df_02 = pd.read_csv('dummies_ds.csv')\n",
    "df_dummies = pd.get_dummies(df_02, columns = [c for c in categorical_cols ])\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a75fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1947da3f",
   "metadata": {},
   "source": [
    "Part 01 Ends Here.  \n",
    "Part 02 will continue in the following link: https://github.com/twpkevin06222/DatiumInsights_DS_Test/blob/main/SectionA/Part02_model_and_evaluation.ipynb  \n",
    "Thank you! :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
